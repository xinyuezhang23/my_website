---
title: "Group assignment 1 of Applied Statics"
date: '2017-10-31T22:42:51-05:00'
description: Group work 1 is about some data visualization using ggplot.
draft: no
image: mountain.jpg
keywords: ''
slug: homework1
categories:
- ''
- ''
---

```{r, setup, echo=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=5.5,
  fig.align = "center"
)
```


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(fivethirtyeight)
library(here)
library(skimr)
library(janitor)
library(vroom)
library(tidyquant)
library(rvest) # to scrape wikipedia page
```



# Where Do People Drink The Most Beer, Wine And Spirits?

Back in 2014, [fivethiryeight.com](https://fivethirtyeight.com/features/dear-mona-followup-where-do-people-drink-the-most-beer-wine-and-spirits/) published an article on alchohol consumption in different countries. The data `drinks` is available as part of the `fivethirtyeight` package. Make sure you have installed the `fivethirtyeight` package before proceeding.
```{r, load_alcohol_data}
# Download data
library(fivethirtyeight)
data(drinks)

```

- What are the variable types? Any missing values we should worry about? 
```{r glimpse_skim_data}
# Skimming the data set to identify missing data and characteristics
glimpse(drinks)
skim(drinks)
```

The data set contains 193 rows and 5 columns. 'country' column has a character data type. 'beer_servings', 'spirit_servings' and 'wine_servings' have an integer data type, while 'total_litres_of_pure_alcohol' has a double data type. None of the columns have any missing values. The data is available for 193 unique countries.

 - Make a plot that shows the top 25 beer consuming countries
```{r beer_plot}
#Matching beer servings drunk per person to each country in descending order and show top 25
drinks %>% 
  slice_max(order_by = beer_servings, n=25) %>% 
  ggplot(
    aes(x=beer_servings,y=fct_reorder(country,beer_servings))) + 
  geom_col(fill='blue') +
  #label graph
  labs(
    title = "Top 25 beer-consuming countries",
    subtitle = "Namibia has the highest beer consumption. \nMost of the top 25 countries belong to Europe.",
    x = "Beer servings drunk per person in 2010",
    y = "Country") +
  theme_bw()
```

 - Make a plot that shows the top 25 wine consuming countries
```{r wine_plot}

#Matching wine servings drunk per person to each country in descending order and show top 25
drinks %>% 
  slice_max(order_by = wine_servings, n=25) %>% 
  ggplot(
    aes(x=wine_servings,y=fct_reorder(country,wine_servings))) + 
  geom_col(fill='blue') +
  #Label graph
  labs(
    title = "Top 25 wine-consuming countries",
    subtitle = "France has the highest wine consumption \nMost of the top 25 countries belong to Europe.",
    x = "Wine servings drunk per person in 2010",
    y = "Country") +
  theme_bw()
```
 
 - Finally, make a plot that shows the top 25 spirit consuming countries
```{r spirit_plot}
#Matching spirit servings drunk per person to each country in descending order and show top 25
drinks %>% 
  slice_max(order_by = spirit_servings, n=25) %>% 
  ggplot(
    aes(x=spirit_servings,y=fct_reorder(country,spirit_servings))) + 
  geom_col(fill='blue') +
  #Label graph
  labs(
    title = "Top 25 spirit-consuming countries",
    subtitle = "Grenada has the highest spirit consumption. \nMost of the top 25 countries belong to Europe and Caribbean region.",
    x = "Spirit servings drunk per person in 2010",
    y = "Country") +
  theme_bw()
```

 - What can you infer from these plots?

The data shows that most of the top 25 countries consuming all three types of alcoholic drinks belong to Europe, inferring that alcohol consumption is very popular in most European countries. Interestingly, spirits are very popular in the Caribbean region while beer and wine are not. In addition, the data displays that some Asian countries such as Kazakhstan and Japan are fond of spirits, while there is almost no Asian country in the top 25 for beer or wine consumption. For all three alcoholic drinks, the increase in consumption in the top 25 is gradual.

# Analysis of movies- IMDB dataset

We will look at a subset sample of movies, taken from the [Kaggle IMDB 5000 movie dataset](https://www.kaggle.com/carolzhangdc/imdb-5000-movie-dataset)
```{r,load_movies, warning=FALSE, message=FALSE, eval=TRUE}

#Loading the dataset
movies <- read_csv(here::here("data", "movies.csv"))

#Examining the structure and summary statistics for the data
glimpse(movies)
skim(movies)
```

## Use your data import, inspection, and cleaning skills to answer the following:

- Are there any missing values (NAs)? Are all entries distinct or are there duplicate entries?

The data has 2961 rows and 11 columns and no missing values. However, it is likely to contain duplicate entries since it has 2961 entries but only 2097 unique movie titles. Similarly, values for genre (17 unique values) and director (1366 unique values) are also repeated. 

- Produce a table with the count of movies by genre, ranked in descending order
```{r, genre_desc}
#See how many movies our data has for each genre
movies %>%
    group_by(genre) %>%
    count(sort=TRUE)
```

- Produce a table with the average gross earning and budget (`gross` and `budget`) by genre. Calculate a variable `return_on_budget` which shows how many $ did a movie make at the box office for each $ of its budget. Ranked genres by this `return_on_budget` in descending order
```{r return_budget}
#Calculate the average gross earning and budget for each genre. Rank the genres by highest return on budget.
movies %>% 
  group_by(genre) %>% 
  
  #Calculating average gross earnings and budget
  summarise(mean_gross = mean(gross),
            mean_budget = mean(budget),
            
            #Calculating the mean of return_on_budget for each genre
            return_on_budget = round(mean((gross/budget)), 2)) %>% 
  
  #Sorting the genres by mean return_on_budget
  arrange(desc(return_on_budget))

```
We observe that Family genre has the highest mean gross while Thriller has the least. Also, the mean budget is the highest for Action movies while its the least for Thriller movies.

For return_on_budget, we see that its the highest for Horror movies, which means that on average, Horror movies make the most $ at the box office for each $ of its budget. On the other hand, Thriller movies have the lowest return_on_budget.

- Produce a table that shows the top 15 directors who have created the highest gross revenue in the box office. Don't just show the total gross amount, but also the mean, median, and standard deviation per director.
```{r 15_directors}
#Rank directors by the highest gross revenue their movies made in total and see the means, medians and standard deviations in gross revenue for each director. 
movies %>% 
  group_by(director) %>% 
  
  #Calculating summary stats for gross movie earnings of each director
  summarize(total_gross = sum(gross),
            mean_gross = mean(gross),
            median_gross = median(gross),
            std_gross = sd(gross)) %>% 
  
  #Sorting the directors by total gross earnings in descending order
  arrange(desc(total_gross)) %>% 
  
  #Picking the top 15 directors according to total gross earnings
  top_n(15, total_gross)
```
Steven Spielberg's movies have had the highest total gross across all directors by a large margin. In terms of mean gross of movies, George Lucas is the most successful director out of these 15. The data displays that out of these 15 directors, James Cameron's movies have had the highest variability in gross collection, Shawn Levy's movies have showed the least variation in gross collection.

- Finally, ratings. Produce a table that describes how ratings are distributed by genre. We don't want just the mean, but also, min, max, median, SD and some kind of a histogram or density graph that visually shows how ratings are distributed. 
```{r ratings_distribution,fig.height=8}
#Examine how IMDB ratings are distributed within various genres. Inspect the summary statistics for each genre.
movies %>% 
  group_by(genre) %>% 
  
  #Calculating summary stats of ratings for movies of each genre
  summarise(mean_rating = mean(rating),
            min_rating = min(rating),
            max_rating = max(rating),
            median_rating = median(rating),
            SD_rating = sd(rating))

#Plotting density plots to see how ratings are distributed
ggplot(movies, aes(x=rating)) +
  geom_density() +
  
  #Faceting the graph by genres
  facet_wrap(~genre, nrow = 5) +
  
  #Adding titles and axes labels
  labs(title = 'Distribution of ratings by genre',
       x = 'IMDB Rating',
       y = 'Density') +
  theme_bw()
```
We observe that movies of Biography genre have the highest mean rating of 7.11. Movies belonging to Musical and Romance genres have the least variability in ratings (each having a standard deviation of 0.636). Although there is only one movie in the Thriller genre, it has the lowest mean rating of 4.8.

## Use `ggplot` to answer the following

  - Examine the relationship between `gross` and `cast_facebook_likes`.
```{r gross_on_fblikes}
#Plotting a scatterplot and regression line for gross earnings vs cast facebook likes
  ggplot(movies, aes(x=cast_facebook_likes, y=gross))+
  geom_point()+
  geom_smooth() +
  
  #Adding titles and axes labels
  labs(title = 'Scatterplot for gross movie earnings vs facebook likes of cast members',
       subtitle = 'No significant pattern can be observed',
       x = 'Number of facebook likes that the cast has received',
       y = 'Gross earnings in the US box office ($)')


#Performing a correlation test between gross and cast facebook likes
cor.test(movies$gross, movies$cast_facebook_likes)
```
We have mapped 'gross' to the y-variable and 'cast_facebook_likes' to the x-variable. As we have to see if number of facebook likes is a good predictor of a movie's gross earnings, 'gross' is the dependent variable (Y) and 'cast_facebook_likes' is the independent variable (X).

'cast_facebook_likes' does not seem to be a good predictor for 'gross' because there is no clear relationship or pattern evident from the scatterplot.

On performing a correlation test, although we observed that the correlation is statistically significant, but the value of correlation is too low (0.213) for 'cast_facebook_likes' to be used as a predictor.
  
  
  - Examine the relationship between `gross` and `budget`. Produce a scatterplot and write one sentence discussing whether budget is likely to be a good predictor of how much money a movie will make at the box office.
```{r gross_on_budget}
#Plotting a scatterplot and regression line for gross earnings vs budget
  ggplot(movies, aes(x=budget, y=gross)) +
  geom_point() +
  geom_smooth() +
  
  #Adding titles and axes labels
  labs(title = 'Scatterplot for gross movie earnings vs movie budget',
       subtitle = 'Positive correlation observed',
       x = 'Movie budget ($)',
       y = 'Gross earnings in the US box office ($)')

#Performing a correlation test between gross and budget
cor.test(movies$gross, movies$budget)
```
We can clearly observe a positive correlation from the scatterplot between a movie's gross earnings and budget. On performing a correlation test, we observed that the correlation is high (0.641) and is statistically significant. Hence, budget is likely to be a good predictor for gross.  
 
  - Examine the relationship between `gross` and `rating`. Produce a scatterplot, faceted by `genre` and discuss whether IMDB ratings are likely to be a good predictor of how much money a movie will make at the box office. Is there anything strange in this dataset?
```{r gross_on_rating,fig.height=7,fig.width=10}
#Plotting a scatterplot and regression line for gross earnings vs rating
  ggplot(movies, aes(x=rating, y=gross))+
  geom_point()+
  geom_smooth() +
  
  #Faceting the graph by genre
  facet_wrap(~genre) +
  
  #Adding titles and axes labels
  labs(title = 'Scatterplot for gross movie earnings vs average IMDB rating',
       subtitle = 'Positive correlation observed for some genres',
       x = 'Average IMDB Rating',
       y = 'Gross earnings in the US box office ($)')

#Performing a correlation test between gross and rating
cor.test(movies$gross, movies$rating)
```
We observe that for some genres such as Action and Adventure, gross earnings and ratings seem to be positively correlated. However, for other genres, the relationship between the two variables does not indicate a strong pattern.

We also see that the correlation is low (0.269) although it is statistically significant. Overall, ratings do not seem to be a good predictor of gross earnings.

Something strange in this dataset is that quite popular genres such as Thriller and Romance are represented by a very small number of movies.

# Returns of financial stocks

```{r load_nyse_data, message=FALSE, warning=FALSE}
nyse <- read_csv(here::here("data","nyse.csv"))
```

  - Based on this dataset, create a table and a bar plot that shows the number of companies per sector, in descending order
```{r companies_per_sector}

#Table of the number of companies per sector, in descending order
nyse %>% 
  group_by(sector)%>%
  count(sort=TRUE)

#Plotting a bar plot for the number of companies per sector
nyse %>% 
  mutate(sector=fct_rev(fct_infreq(sector))) %>% 
  ggplot(aes(y=sector)) +
  geom_bar() +

#Adding titles and axes labels
  labs(title = 'Number of companies per sector',
       subtitle = 'Finance has the largest no. of companies while Consumer Durables has the least',
       x = 'Count of companies',
       y = 'Sector') +
  theme_bw()+
  theme(plot.subtitle=element_text(size=6))

```

```{r get_price_data, message=FALSE, warning=FALSE, cache=TRUE}
# Notice the cache=TRUE argument inthe chunk options. Because getting data is time consuming, 
# cache=TRUE means that once it downloads data, the chunk will not run again next time you knit your Rmd

#Geting data for 6 different stocks and 'SPY'
myStocks <- c("PLOW","GOOGL","ZEUS","ORCL","FUN","ROCK","SPY" ) %>%
  tq_get(get  = "stock.prices",
         from = "2011-01-01",
         to   = "2021-08-31") %>%
  group_by(symbol)

glimpse(myStocks) # examine the structure of the resulting data frame
```

```{r calculate_returns, message=FALSE, warning=FALSE, cache=TRUE}
#calculate daily returns
myStocks_returns_daily <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "daily", 
               type       = "log",
               col_rename = "daily_returns",
               cols = c(nested.col))  

#calculate monthly  returns
myStocks_returns_monthly <- myStocks %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "monthly", 
               type       = "arithmetic",
               col_rename = "monthly_returns",
               cols = c(nested.col)) 

#calculate yearly returns
myStocks_returns_annual <- myStocks %>%
  group_by(symbol) %>%
  tq_transmute(select     = adjusted, 
               mutate_fun = periodReturn, 
               period     = "yearly", 
               type       = "arithmetic",
               col_rename = "yearly_returns",
               cols = c(nested.col))
```
  - Create a table where you summarise monthly returns for each of the stocks and `SPY`; min, max, median, mean, SD.
```{r summarise_monthly_returns}
#Calculating summary stats of monthly returns for each stock
myStocks_returns_monthly %>% 
  group_by(symbol) %>% 
  summarise_each(
    funs(min, max, median, mean, sd), monthly_returns
    )
```
It can be observed that GOOGL has the highest mean (0.0198) and median (0.01774) monthly returns out of all the stocks. The lowest mean monthly return is for ORCL (0.0109).

In terms of standard deviation of monthly returns, ZEUS is the most risky stock as its standard deviation is highest, while SPY is the least risky.

Plot a density plot, using `geom_density()`, for each of the stocks.
```{r density_monthly_returns}
#Density plot for monthly returns
ggplot(myStocks_returns_monthly,
       aes(x=monthly_returns)) +
  geom_density() +
  
  #Faceting the plot by symbols
  facet_wrap(~symbol) +
  
  #Adding title and axes labels
  labs(title = 'Monthly Returns of Stocks',
       x = 'Monthly returns',
       y = 'Density')


```

  - What can you infer from this plot? Which stock is the riskiest? The least risky? 

The distribution of the monthly returns for all the mentioned stocks follow an approximately normal distribution.
We observe that the riskiest stock is ZEUS as the distribution of its returns has fat tails and the standard deviation of returns is the highest (0.1683). On the other hand, the least risky stock is SPY as the returns are very concentrated around the mean and the standard deviation is the least (0.0381).

  - Finally, make a plot that shows the expected monthly return (mean) of a stock on the Y axis and the risk (standard deviation) in the X-axis. Please use `ggrepel::geom_text_repel()` to label each stock

```{r risk_return_plot}
myStocks_returns_monthly %>% 
  group_by(symbol) %>% 
  
  #Calculating mean and std dev of monthly returns for each stock
  summarise_each(
    funs(mean, sd), monthly_returns
    ) %>% 
  
  #Plotting expected monthly return on the Y Axis and risk on the X-axis
  ggplot(aes(x=sd,y=mean,color=symbol)) +
  geom_point() + 
  
  #Labeling each stock
  ggrepel::geom_text_repel(aes(label=symbol)) +
  
  #Adding title and axis labels
  labs(title = 'Risk vs Returns',
       y = 'Mean monthly returns',
       x = 'Standard Deviation of monthly returns',
       color="Stock") + theme_bw()
```

What can you infer from this plot? Are there any stocks which, while being riskier, do not have a higher expected return?

We observe that GOOGL seems to be the best investment out of these stocks as it has the highest return and lower risk than most other stocks. 
ZEUS seems to be a bad investment as the risk is highest but the returns are on the lower side.

For a similar level of risk, GOOGL gives a much higher return than ORCL. Additionally, ROCK gives a higher return than FUN for a similar level of risk.


# On your own: IBM HR Analytics


```{r data_load_IBM}

hr_dataset <- read_csv(here::here("data", "datasets_1067_1925_WA_Fn-UseC_-HR-Employee-Attrition.csv"))
glimpse(hr_dataset)

```

```{r hr_cleaned}

hr_cleaned <- hr_dataset %>% 
  clean_names() %>% 
  mutate(
    education = case_when(
      education == 1 ~ "Below College",
      education == 2 ~ "College",
      education == 3 ~ "Bachelor",
      education == 4 ~ "Master",
      education == 5 ~ "Doctor"
    ),
    environment_satisfaction = case_when(
      environment_satisfaction == 1 ~ "Low",
      environment_satisfaction == 2 ~ "Medium",
      environment_satisfaction == 3 ~ "High",
      environment_satisfaction == 4 ~ "Very High"
    ),
    job_satisfaction = case_when(
      job_satisfaction == 1 ~ "Low",
      job_satisfaction == 2 ~ "Medium",
      job_satisfaction == 3 ~ "High",
      job_satisfaction == 4 ~ "Very High"
    ),
    performance_rating = case_when(
      performance_rating == 1 ~ "Low",
      performance_rating == 2 ~ "Good",
      performance_rating == 3 ~ "Excellent",
      performance_rating == 4 ~ "Outstanding"
    ),
    work_life_balance = case_when(
      work_life_balance == 1 ~ "Bad",
      work_life_balance == 2 ~ "Good",
      work_life_balance == 3 ~ "Better",
      work_life_balance == 4 ~ "Best"
    )
  ) %>% 
  select(age, attrition, daily_rate, department,
         distance_from_home, education,
         gender, job_role,environment_satisfaction,
         job_satisfaction, marital_status,
         monthly_income, num_companies_worked, percent_salary_hike,
         performance_rating, total_working_years,
         work_life_balance, years_at_company,
         years_since_last_promotion)

```

  - Produce a one-page summary describing this dataset
  
*We will employ a number of graphs and different statistics to examine how some key metrics affect salary. We will also take an overall look on employee satisfaction and examine how employees perceive their employment. Hopefully this will give us a meaningful insight on the company's working environment *

Lets begin by seeing how employees relate to their job:

Firstly we will take a look at two key metrics: 
1. Employee job satisfaction
2. Employee work life balance

to do that we plot poll results from within the company:

```{r}
library(scales)
#Job Satisfaction
#create table showing count of every satisfaction level 
satisfaction_table<-hr_cleaned %>%
   mutate(job_satisfaction=fct_infreq(job_satisfaction),N=n()) %>%
   group_by(job_satisfaction) %>%
   mutate(count=n(),per=percent(count/N,0.1)) %>% 
  select(job_satisfaction,count,per) %>% 
  unique() #remove duplicates

#display table 
satisfaction_table

ggplot(satisfaction_table, aes(x=fct_rev(reorder(job_satisfaction,count)),y=count,fill=job_satisfaction))+
  geom_col(width=0.3)+geom_text(aes(label=per),nudge_y = 15,size=3,colour = "black")+
   #edit theme, text size exc.
  theme(axis.text =element_text(size=10),axis.title = element_text(size=10),legend.text = element_text(size=10) ,legend.title = element_text(size=10),legend.box.background = element_rect()) +
  #add labels
  labs(
     title = "Job Satisfaction Ratings",
     subtitle = "Employees seem satisfied with their working enviroment in general",
     x = "Satisfaction Rating",
     y = "Count",
     fill="Job Satisfaction")

#Work Life Balance

#create table showing count of every worklife balance level 
life_balance_table<-hr_cleaned %>%
   mutate(N=n()) %>%
   group_by(work_life_balance) %>%
   mutate(count=n(),per=percent(count/N,0.1)) %>% 
   select(work_life_balance,count,per) %>% 
   unique()

#display table 
life_balance_table

ggplot(life_balance_table, aes(x=fct_rev(reorder(work_life_balance,count)),y=count,fill=work_life_balance))+
  geom_col(width=0.3)+geom_text(aes(label=per),nudge_y = 25,size=3,colour = "black")+
  #edit theme, text size exc.
  theme(axis.text =element_text(size=10),axis.title = element_text(size=10),legend.text = element_text(size=10) ,legend.title = element_text(size=10),legend.box.background = element_rect())+
  #add labels
  labs(
     title = "Work Life Balance Ratings",
     subtitle = "Employees seem satisfied with their work life balance in general",
     x = "Work Life Balance Rating",
     y = "Count",
     fill="Rating")
```
*Job Satisfaction: Over 60% of the sample have either a job satisfaction of very high or high while 20% have a job satisfaction of medium and low respectively.* 
*Work Life Balance: While 60% have a "better" work-life balance 23% have a "good" work-life balance, 10% have the "best" work-life balance and only 5% have a "bad" work-life balance.*

The above figures indicate a healthy working environment with minimal employee dissatisfaction.

Going forward it would be interesting to see how this perceived as a happy working environment measures against employee turnover. We will examine how probable it is for an employee to change employment depending on his years in the company

```{r }
# find when employees quit (descriptive statistics)
Quit_employees<-hr_cleaned %>% 
  filter(attrition=='Yes')
Quit_employees%>% 
  summarise_each(funs(mean,max,min,median,sd),years_at_company)

#create density graph

hr_cleaned %>% 
  filter(attrition =="Yes") %>% 
  ggplot(aes(x=years_at_company))+geom_density()+
  labs(
    title = "Density Plot",
    subtitle = "How many years people spent in the company before leaving",
    x = "Years employed",
    y = "Density Function")
```
We can tell from this graph that people tend to leave between the 0th and 12th years of work. Among them, the first 2-3 years are the peak period of resignation. The number of quits is relatively small if people have worked for more than 15 years.

It seems like people who are not a good fit, find out soon enough and change environments. Employees who commit for more years on the other hand , seem to stick to their choice, with people leaving after more than 10 years within company ranks being considerably fewer.

Then we will examine how employee age, years at the company, monthly income, and years since last promotion, are distributed and examine the descriptive statistics in order to gain more insights:

```{r}
skim(list(hr_cleaned$age,hr_cleaned$years_at_company,hr_cleaned$monthly_income,hr_cleaned$years_since_last_promotion))

# age fits into the normal distribution
```

By looking at the summary statistics, we can observe that the variable 'age' is closer to normal distribution as the difference between p25 and p50 (6), and p50 and p75 (7) is similar. Additionally, the difference between p0 and p50 (18), and p50 and p100 (24) is close. This means that the distribution is somewhat symmetrical and hence, closer to normal distribution than others. People seem to be getting promotions, on average, every 2 years (although there is a quite the variability), which might explain high rates of satisfaction. Average time at the company is 7 years (again sd is quite large) but also indicates low turnover rates.

As a final measure of the company's standing we will examine how salaries vary for women compared to men, and comment on the results:

```{r, gender}
#create boxplots showing distribution of salaries, for males and females

#create table with desired data
hr_cleaned %>% 
  group_by(gender) %>%
  summarise_each(funs(min,max,median,mean,sd),
                 monthly_income) %>% 
  arrange(desc(mean))
  
#plot the boxplots
hr_cleaned %>% 
  ggplot(aes(x=fct_reorder(gender,monthly_income),y= monthly_income,fill=gender))+
  geom_boxplot(width=0.5)+
  #add labels
  labs(title='Monthly income and Gender',
       x='Gender',
       y='Monthly income',
       fill='Gender')+theme(axis.title = element_text(size=15))
```

Females seem to be making slightly more money than males on this company. Even though median salaries are very close (with female employees showing only a slight lead) it can be seen that essentially every quartile is higher for women than for men, in particular the 4th. On the other hand high earners of both genders, as seen by outliers, earn similar salaries. However differences are not high enough to constitute discrimination, and it seems like the company has done an exceptional job on creating an equal and fair working environment. This could very well be another reason employee satisfaction is so high, as it is not uncommon to observe discrimination when it comes to salaries.

We have shown that gender doesn't play a big role in determining monthly income, but what else could be an important factor?

Lets take a look at how one's educational background, relate to monthly income:
```{r}
#RELATIONSHIP BETWEEN MONTHLY INCOME AND EDUCATION
hr_cleaned %>% 
  group_by(education) %>%
  summarise_each(funs(min,max,median,mean,sd),
                 monthly_income) %>% 
  arrange(desc(mean))
  
hr_cleaned %>% 
  ggplot(aes(x = reorder(education, monthly_income), y = monthly_income, fill=education)) +
  geom_boxplot() +
  #add labels
  labs(title = 'Distribution of Monthly Income and Education',
       subtitle = 'We observe that higher education leads to higher monthly income on average.',
       x = 'Education level',
       y = 'Monthly Income',
       fill='Education') +
  theme_bw()
```
We observe a clear pattern in the relationship between Monthly income and Education. On average, a higher educated employee earns more than a lesser educated employee. Although we see that the median income of Bachelor is slightly lower than that of College, the mean income of Bachelor is higher than that for College.
We also obverse that many outliers who have education till Below College, College, Bachelor or Master level earn almost as much as the maximum paid Doctor.
Since we have outliers for almost all education levels, median would be the better central tendency measure to look at instead of a mean as median is not much affected by outliers.

We will persist on the matter of education level and how it relates to to income level. We will create a bar chart showing the median of salaries for different educational backgrounds. Median is preferred over mean for salaries as it is more robust to outliers.
```{r}
#create boxplots for salaries as they reate to different educational backgrounds
hr_cleaned %>% 
  group_by(education) %>% 
  summarise(median_income=median(monthly_income)) %>% 
  arrange(desc(median_income))

hr_cleaned %>% 
  group_by(education) %>% 
  summarise(median_income=median(monthly_income)) %>% 
  mutate(education=fct_reorder(education,median_income)) %>% 
  ggplot(aes(x=education,y=median_income,fill=education))+
   #edit theme, text size exc.
  geom_col(width=0.5)+geom_text(aes(label=round(median_income)),colour='black',nudge_y = 190,size=3)+
  theme(axis.text =element_text(size=8),axis.title = element_text(size=8) ) +
  #add labels
  labs(title = 'Barchart of Monthly Income vs Educational Background',
        x = 'Education',
        y = 'Median Income',
       fill='Education')
```

Not surprisingly PhD holders are the highest earners.

In order to more closely examine how educational level relates to income we will also plot density plots.

```{r}
#create density plots for salary as it relates to educational level
ggplot(hr_cleaned,aes(x=monthly_income,color=education))+
  geom_density(alpha=0)+
  #facet for education level
  facet_wrap(~education)+
  theme_economist()+
   #edit theme, text size exc.
  theme(legend.position = "none",axis.text =element_text(size=5),axis.title = element_text(size=5,hjust = 0.5) ,strip.text=element_text(size=5),axis.title.x.bottom =   element_text(margin=margin(15,0,0,0)),axis.title.y.left = element_text(margin=margin(0,15,0,0)),plot.title = element_text(hjust = 0.5,size=9,margin=margin(0,0,7,0)))+
  #add labels
  labs(title = 'Density plots of Monthly Income vs Education',
        x = 'Monthly Income',
        y = 'Density Function')


```
As what have been interpreted before, people are more likely to have higher monthly wages if they have been educated for a longer period. Besides, all these five density plots are right-skewed, so there are other factors that help to differentiate earnings between people having similar education backgrounds. The plot also looks a lot better when we use the economist graph format.


Finally it is time to examine how different job roles are compensated within the company, as one would expect this to be the most important factor:

```{r,fig.height=4}
#examine incomes as they relate to job titles

hr_cleaned %>% 
  mutate(job_role = fct_reorder(job_role, monthly_income)) %>% 
  ggplot(aes(x = monthly_income, y = job_role, 
             fill = job_role)) +
  geom_boxplot() +
  #add labels
  labs(title = 'Boxplot of Monthly Income vs Job Role',
        x = 'Monthly Income',
        y = 'Job Role',
       fill='Job Role') +
  theme_bw()
```
It can be observed that the median monthly income of a Manager is the highest while that of a Sales Representative is the lowest. Managers are closely followed by Research directors and then there is a large income gap between Research Directors and Healthcare Representative.

We also observe that job roles with lower monthly income have less spread as the data is more concentrated towards the median and the interquartile range is low. On the other hand, job roles with higher monthly income have more spread out values and the interquartile range is high.

To gain a little more insight on how job role and age in particular affect monthly compensations we will plot a few more scatter plots, one for each job title:

```{r, fig.height=15,fig.width=10}

# scatter plots with regression line, for income as it relates to age and job role
ggplot(hr_cleaned,aes(x=age,y=monthly_income))+
  geom_point()+
  geom_smooth()+
  #facet for job role
  facet_wrap(~job_role,ncol=3)+
  theme(legend.position = 'none',axis.text =element_text(size=15),axis.title = element_text(size=15) ,strip.text=element_text(size=10) )

```

Overall there is an increase in income with older age. Managers and Research Directors display the highest income, however, there is only data for older age implying that these are more senior positions. Interestingly there are also quite a few positions where towards older age the monthly income is declining again. "Human Resources" has the greatest variability while "Research Assistant" and "Sales Representatives" show a more narrow distribution.

# Challenge 1: Replicating a chart

```{r challenge1, echo=FALSE, out.width="90%"}
knitr::include_graphics(here::here("images", "vaxxes_by_state_red_blue_every_county_070321_1.jpeg"), error = FALSE)
```

```{r, echo=FALSE, cache=TRUE}

# Download CDC vaccination by county
cdc_url <- "https://data.cdc.gov/api/views/8xkx-amqh/rows.csv?accessType=DOWNLOAD"
vaccinations <- vroom(cdc_url) %>% 
  janitor::clean_names() %>% 
  filter(fips != "UNK") # remove counties that have an unknown (UNK) FIPS code

# Download County Presidential Election Returns
# https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/VOQCHQ
election2020_results <- vroom(here::here("data", "countypres_2000-2020.csv")) %>% 
  janitor::clean_names() %>% 
  
  # just keep the results for the 2020 election
  filter(year == "2020") %>% 
  
  # change original name county_fips to fips, to be consistent with the other two files
  rename (fips = county_fips)

# Download county population data
population_url <- "https://www.ers.usda.gov/webdocs/DataFiles/48747/PopulationEstimates.csv?v=2232"
population <- vroom(population_url) %>% 
  janitor::clean_names() %>% 
  
  # select the latest data, namely 2019
  select(fips = fip_stxt, pop_estimate_2019) %>% 
  
  # pad FIPS codes with leading zeros, so they are always made up of 5 characters
  mutate(fips = stringi::stri_pad_left(fips, width=5, pad = "0"))


         


```

```{r echo=T, results='hide'}
# We choose the latest date as the day that we are going to analyze
vaccinations_cleaned<-vaccinations %>% 
  filter(date=="09/03/2021")

# Filtering the vaccination data to focus on how many % votes Donald Trump had secured, and calculating his support rate.
election_cleaned<-election2020_results %>% 
  mutate(support_rate=candidatevotes/totalvotes) %>% 
  filter(candidate=="DONALD J TRUMP") %>% 
  group_by(fips) %>% 
  mutate(support_rate=sum(support_rate)) %>% 
  select(fips,county_name,support_rate) %>% 
  unique()

# Merging all three tables together into one for graphing later.
vacc_election<-left_join(vaccinations_cleaned,election_cleaned,by="fips")%>% 
  select(fips,county_name,series_complete_pop_pct,support_rate) 
vacc_election_pop<-left_join(vacc_election,population,by="fips") %>% 
  rename(pop=pop_estimate_2019,vacc_rate=series_complete_pop_pct) %>% 
  filter(vacc_rate>=5) %>% 
  mutate(vacc_rate=vacc_rate/100)

# Carrying out a linear regression analysis and writing down the formula
fit <- lm(vacc_rate ~ support_rate, data=vacc_election_pop)
summary(fit)

```

```{r,fig.height=12,fig.width=8}
# Making the basic scatterplot
ggplot(vacc_election_pop,aes(x=support_rate,y=vacc_rate,size=pop))+
  geom_point(alpha=0.6,colour='black')+
# Some exterior fixings
  theme(legend.position = "none",plot.title=element_text(hjust=0.5,size=15,face="bold"),plot.subtitle = element_text(hjust = 0.5,size=10),axis.text =element_text(size=5),axis.title = element_text(size=5))+
# Set the scale of our axis
  scale_y_continuous(labels = scales::percent_format(accuracy = 1),breaks=seq(0,1,by=0.05),limit=c(0,1.05))+
  scale_x_continuous(labels = scales::percent_format(accuracy = 1),breaks=seq(0,1,by=0.05),limit=c(0,1))+
# Adding all the texts
  labs(title = "COVID-19 VACCINATION LEVELS OUT OF TOTAL POPULATION BY COUNTY",
         subtitle = "(most states based on FULLY vaccinated only;CA,GA,IA,MI & TX based on total doses administered) \n Data via Centers for Disease Control, COVID Act Now, state health depts \n Graph by Study Group 4",
         x = "2020 Trump Vote %", 
         y = "% of Total Population Vaccinated")+
# Changing the background 
  annotate("text",x=0.5,y=1.05,label="EVERY U.S. COUNTY",size=7,color='black',face='bold')+geom_smooth(method='lm',formula=y~x,se=FALSE,alpha=0.1,linetype="dashed")+
  annotate("text",x=0.1,y=0.035,label="y=-0.4173x+0.6601",color='red',face='bold',size=3)+
  annotate("text",x=0.1,y=0.01,label="R squared=0.272",color='red',face='bold',size=3)+
  annotate("text",x=0.3,y=0.02,label="09/03/2021",color='red',face='bold',size=3)+
  geom_hline(yintercept=0.495, linetype="dashed", color = "black")+
  geom_hline(yintercept=0.539, linetype="dashed", color = "black")+
  geom_hline(yintercept=0.85, linetype="dashed", color = "black")+
  annotate("text",x=0.01,y=0.501,label="Actual:49.5%",color='blue',face='bold',size=3)+
  annotate("text",x=0.01,y=0.545,label="Target:53.9%",color='blue',face='bold',size=3)+
  annotate("text",x=0.02,y=0.856,label="Herd Immunity threshold (?)",color='blue',face='bold',size=3)+
  annotate("rect", xmin=-Inf, xmax=0.55, ymin=-Inf, ymax=Inf, alpha=0.2, fill="blue")+
  annotate("rect", xmin=0.45, xmax=Inf, ymin=-Inf, ymax=Inf, alpha=0.2, fill="red")
```

Although we get a different regression result, the conclusions drawn from it are the same as the original article. We believe this graph does the right job.

# Challenge 2: Opinion polls for the 2021 German elections

```{r, scrape_wikipedia_polling_data, warnings= FALSE, message=FALSE}
url <- "https://en.wikipedia.org/wiki/Opinion_polling_for_the_2021_German_federal_election"
# https://www.economist.com/graphic-detail/who-will-succeed-angela-merkel
# https://www.theguardian.com/world/2021/jun/21/german-election-poll-tracker-who-will-be-the-next-chancellor


# get tables that exist on wikipedia page 
tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")


# parse HTML tables into a dataframe called polls 
# Use purr::map() to create a list of all tables in URL
polls <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names())


# list of opinion polls
german_election_polls <- polls[[1]] %>% # the first table on the page contains the list of all opinions polls
  slice(2:(n()-1)) %>%  # drop the first row, as it contains again the variable names and last row that contains 2017 results
  mutate(
         # polls are shown to run from-to, e.g. 9-13 Aug 2021. We keep the last date, 13 Aug here, as the poll date
         # and we extract it by picking the last 11 characters from that field
         end_date = str_sub(fieldwork_date, -11),
         
         # end_date is still a string, so we convert it into a date object using lubridate::dmy()
         end_date = dmy(end_date),
         
         # we also get the month and week number from the date, if we want to do analysis by month- week, etc.
         month = month(end_date),
         week = isoweek(end_date)
         )



```

```{r}
nrow(german_election_polls)

#Removing duplicate values
german_election_polls <- german_election_polls[!duplicated(german_election_polls), ]

nrow(german_election_polls)
```

```{r}
#Calculating rolling average for different political parties

german_election_polls_avg <- german_election_polls %>%
  mutate(
    union_avg = rollmean(union, 14, align="left", fill = NA),
    spd_avg = rollmean(spd, 14, align="left", fill = NA),
    afd_avg = rollmean(af_d, 14, align="left", fill = NA),
    fdp_avg = rollmean(fdp, 14, align="left", fill = NA),
    linke_avg = rollmean(linke, 14, align="left", fill = NA),
    grune_avg = rollmean(grune, 14, align="left", fill = NA)
    )
```

```{r,fig.width=9,fig.height=7}
Sys.setlocale(locale="en_US.UTF-8")
#Plotting the graph

#Setting colors for different political parties
color_parties<-c("SPD"="red3","CDU/CSU"="black","Grüne"="green3","FDP"="orange","AfD"="blue3","Linke"="purple3")

german_election_polls_avg %>%
  ggplot +

  #Union
  geom_point(aes(x=end_date,y=union,colour="CDU/CSU"), alpha=0.5)+
  geom_line(aes(x=end_date,y=union_avg,colour="CDU/CSU"),
            se=FALSE,size=1.1, alpha=0.6)+
  #spd
  geom_point(aes(x=end_date,y=spd,colour="SPD"), alpha=0.5)+
  geom_line(aes(x=end_date,y=spd_avg,colour="SPD"),
            se=FALSE,size=1.1, alpha=0.6)+
  #af_d
  geom_point(aes(x=end_date,y=af_d,colour="AfD"), alpha=0.5)+
  geom_line(aes(x=end_date,y=afd_avg,colour="AfD"),
            se=FALSE,size=1.1, alpha=0.6)+
  #fdp
  geom_point(aes(x=end_date,y=fdp,colour="FDP"), alpha=0.5)+
  geom_line(aes(x=end_date,y=fdp_avg,colour="FDP"),
            se=FALSE,size=1.1, alpha=0.6)+
  #linke
  geom_point(aes(x=end_date,y=linke,colour="Linke"), alpha=0.5)+
  geom_line(aes(x=end_date,y=linke_avg,colour="Linke"),
            se=FALSE,size=1.1, alpha=0.6)+
  #grune
  geom_point(aes(x=end_date,y=grune,colour="Grüne"), alpha=0.5)+
  geom_line(aes(x=end_date,y=grune_avg,colour="Grüne"),
            se=FALSE,size=1.1, alpha=0.6)+

  scale_colour_manual(name="09/03/2021",values=color_parties)+

  #Setting x and y axes labels to be null
  labs(x="", y="") +

  #Date labels for x-axis
  scale_x_date(date_minor_breaks = "1 months",
               date_labels = "%b %Y") +

  #Percentage labels for y-axis
  scale_y_continuous(breaks = c(5, 15, 25, 35, 45),
                     labels = function(x) paste0(x, "%")) +

  #Setting black and white theme, and removing grid lines
  theme_bw() +
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),legend.title = element_text(size=10,face="bold"),legend.box.background = element_rect(),legend.position=c(0.99,0.99),legend.justification = c("right", "top")) +

  #Plotting horizontal lines
  geom_hline(aes(yintercept = 5), linetype="dashed", alpha=0.2) +
  geom_hline(aes(yintercept = 15), linetype="dashed", alpha=0.2) +
  geom_hline(aes(yintercept = 25), linetype="dashed", alpha=0.2) +
  geom_hline(aes(yintercept = 35), linetype="dashed", alpha=0.2)
```

# Details

- Who did you collaborate with: Study Group 4
- Approximately how much time did you spend on this problem set: 10 hours
- What, if anything, gave you the most trouble: Two challenges
